{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "9Bd_bzNC4NdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vcufg3SohJGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('waterQuality1.csv')\n",
        "x = dataset.iloc[: , :-1].values\n",
        "y = dataset.iloc[: , -1].values\n"
      ],
      "metadata": {
        "id": "QfEgBs274xB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "id": "GG_NweEc5isF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "x_train[:, 3:] = sc.fit_transform(x_train[:, 3:])\n",
        "x_test[:, 3:] = sc.transform(x_test[:, 3:])"
      ],
      "metadata": {
        "id": "eZiSKPuX-La7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "id": "ARhrPNM25k1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "id": "XkKpfXt67sPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure all relevant columns are numeric, coercing errors to NaN\n",
        "# Assuming the first column is the one causing issues and needs encoding,\n",
        "# but converting all columns to numeric is a safer general approach\n",
        "for col in dataset.columns[:-1]: # Exclude the target variable column\n",
        "    dataset[col] = pd.to_numeric(dataset[col], errors='coerce')\n",
        "\n",
        "# Separate features (x) and target (y) again after cleaning\n",
        "x = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values\n",
        "\n",
        "\n",
        "# Impute missing values in x\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "x_imputed = imputer.fit_transform(x)\n",
        "\n",
        "\n",
        "# Apply OneHotEncoder to the first column and pass through the rest\n",
        "# The first column was already identified as potentially requiring encoding based on previous attempts.\n",
        "# Ensure the index [0] corresponds to the column needing one-hot encoding\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
        "x_encoded = ct.fit_transform(x_imputed)\n",
        "\n",
        "# Convert the sparse matrix to dense if necessary (OneHotEncoder outputs sparse by default)\n",
        "if hasattr(x_encoded, 'toarray'):\n",
        "    x = x_encoded.toarray()\n",
        "else:\n",
        "    x = x_encoded # Already dense\n",
        "\n",
        "print(x)"
      ],
      "metadata": {
        "id": "tc-sEypv87BW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "id": "VFDjcivZ9lUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "id": "3J8SR2su9ocU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ],
      "metadata": {
        "id": "FDgUl3Ta9wvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "id": "lbffiZbz97dN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dc886dd"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# Apply feature scaling to all columns in the training and testing sets\n",
        "sc = StandardScaler()\n",
        "x_train = sc.fit_transform(x_train)\n",
        "x_test = sc.transform(x_test)\n",
        "\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"x_test shape:\", x_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c138d11"
      },
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure all relevant columns are numeric, coercing errors to NaN\n",
        "for col in x.columns:\n",
        "    x[col] = pd.to_numeric(x[col], errors='coerce')\n",
        "\n",
        "# Impute missing values\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "x_imputed = imputer.fit_transform(x)\n",
        "\n",
        "# Apply OneHotEncoder to the first column and pass through the rest\n",
        "# The first column was already identified as potentially requiring encoding based on previous attempts.\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
        "x_encoded = ct.fit_transform(x_imputed)\n",
        "\n",
        "# Convert the sparse matrix to dense if necessary (OneHotEncoder outputs sparse by default)\n",
        "if hasattr(x_encoded, 'toarray'):\n",
        "    x_dense = x_encoded.toarray()\n",
        "else:\n",
        "    x_dense = x_encoded # Already dense\n",
        "\n",
        "# Split the data\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_dense, y, test_size=0.2, random_state=1)\n",
        "\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"x_test shape:\", x_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train)"
      ],
      "metadata": {
        "id": "RVlVrqa-_N21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_test)"
      ],
      "metadata": {
        "id": "YabxhmSW_Sq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train)"
      ],
      "metadata": {
        "id": "Vytd0z1z_WM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test)"
      ],
      "metadata": {
        "id": "k9Z5upGJ_Ycz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "x_train[:, 3:] = sc.fit_transform(x_train[:, 3:])\n",
        "x_test[:, 3:] = sc.transform(x_test[:, 3:])"
      ],
      "metadata": {
        "id": "YN0NgVPJ_pnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train)\n"
      ],
      "metadata": {
        "id": "rXQDHsha_zss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_test)"
      ],
      "metadata": {
        "id": "QzJwL96T_23z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Week 2 â€“ Machine Learning Model with EDA, Transformation, Feature Selection, and Accuracy\n",
        "\n",
        "# ===========================\n",
        "# 1. Import Libraries\n",
        "# ===========================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier  # For classification problems\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# ===========================\n",
        "# 2. Load Dataset\n",
        "# ===========================\n",
        "# Replace with your actual dataset name\n",
        "data = pd.read_csv('/content/waterQuality1 (1).csv')\n",
        "print(\"First 5 rows of data:\")\n",
        "print(data.head())\n",
        "print(\"\\nDataset Info:\")\n",
        "print(data.info())\n",
        "\n",
        "# Convert 'ammonia' and 'is_safe' columns to numeric, coercing errors\n",
        "data['ammonia'] = pd.to_numeric(data['ammonia'], errors='coerce')\n",
        "data['is_safe'] = pd.to_numeric(data['is_safe'], errors='coerce')\n",
        "\n",
        "# Drop rows where 'ammonia' is NaN (which resulted from coercion)\n",
        "data.dropna(subset=['ammonia'], inplace=True)\n",
        "\n",
        "# ===========================\n",
        "# 3. Exploratory Data Analysis (EDA)\n",
        "# ===========================\n",
        "print(\"\\nChecking for missing values:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Correlation Heatmap\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(data.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# Visualize distributions\n",
        "data.hist(figsize=(12,8))\n",
        "plt.suptitle(\"Feature Distributions\")\n",
        "plt.show()\n",
        "\n",
        "# ===========================\n",
        "# 4. Data Transformation\n",
        "# ===========================\n",
        "# Fill missing values (example: using mean for numeric)\n",
        "data = data.fillna(data.mean())\n",
        "\n",
        "# Encode categorical columns (if any)\n",
        "data = pd.get_dummies(data, drop_first=True)\n",
        "\n",
        "print(\"\\nData after transformation:\")\n",
        "print(data.head())\n",
        "\n",
        "# ===========================\n",
        "# 5. Feature Selection\n",
        "# ===========================\n",
        "# Replace 'target_column' with your actual target variable name\n",
        "target = 'is_safe' # Updated target column name\n",
        "X = data.drop(target, axis=1)\n",
        "y = data[target]\n",
        "\n",
        "# Quick feature importance using Random Forest\n",
        "temp_model = RandomForestClassifier(random_state=42)\n",
        "temp_model.fit(X, y)\n",
        "feature_importances = pd.Series(temp_model.feature_importances_, index=X.columns)\n",
        "feature_importances.nlargest(10).plot(kind='barh')\n",
        "plt.title(\"Top 10 Important Features\")\n",
        "plt.show()\n",
        "\n",
        "# ===========================\n",
        "# 6. Model Training\n",
        "# ===========================\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# ===========================\n",
        "# 7. Accuracy Evaluation\n",
        "# ===========================\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nModel Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KF4PPafzhM-w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}